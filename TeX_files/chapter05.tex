\chapter{R-process}\label{chap:rprocess}

I'll say some stuff about r-process nuclei here

Fission inputs to r-process network calculations

You should definitely cite the 2015 paper by Eichler et al (doi:10.1088/0004-637X/808/1/30), titled ``THE ROLE OF FISSION IN NEUTRON STAR MERGERS AND ITS IMPACT ON THE r-PROCESS PEAKS''. The region of largest variation (in their conclusions, at least) is in the region A=100-160, which makes sense because that's where most of your fission fragments are likely to lie. This is roughly-speaking also the region of that rare-earth peak everyone always talks about, along with the so-called second r-process peak.

Kilonova - the bright burst of gamma rays and other radiation that accompanies a compact object merger, presumably created mainly from the radioactive decay of unstable r-process nuclei (mainly via alpha and beta decay, but not fission - according to \verb|https://www.nndc.bnl.gov/fire/documents/FIRE_DOE_Yonglin.pdf|; see also the paper \cite{Zhu2018})

Light curves - show the magnitude/intensity of light/EM radiation as a function of time. For example, the light curve from the sun on the earth will be roughly sinusoidal; from an eclipse, you'll have a roughly straight line, then a dip, then a return to the original straight line; a pulsar will also be something regular and periodic. Each type of nova/supernova/kilonova/etc. has its own characteristic light curve, with an initial peak of varying sharpness, and then a gradual decay (though how gradual depends on the characteristics of the event).

Heating - Is exactly what it sounds like. When a nucleus decays, it loses energy. Some of that energy escapes in the form of neutrinos or photons, while other energy is absorbed elsewhere in the medium. A related concept is opacity. Heavier nuclei with a large level density tend to be more opaque because they can more readily absorb photons than smaller, less opaque nuclei. And the process by which all of this energy exchange takes place is called thermalization.

The idea behind this {\Cf} calculation is related to \cite{Zhu2018}: There is some speculation that the heating from the NSM might be strongly-impacted by spontaneous fission of {\Cf}. A related idea that {\Cf} may have been a large contributor to supernova (not kilonova) lightcurves dates back to 1956 (see references in Zhu's introduction), but perhaps fell out of favor once it was discovered that the decay chain of 56Ni was the primary contributor.

Mass and kinetic energy distributions of {\Cf} were actually measured in \cite{Brandt1963}. For some reason, though, Zhu considers that measurement ``sparse'' and they do some dressing up of it in their paper.

Since {\Cf} is heavier than actinides, detecting its presence in kilonovae would help solidify the location of \textit{r} process nucleosynthesis. And if it has as large an impact on the heating and light curves as \cite{Zhu2018} says, it might well make the difference between ``hard-to-observe'' and ``somewhat easier to observe.''

So what was it about the fission of {\Cf} that \cite{Zhu2018} needed in order to make this argument? And how did they zero-in on {\Cf} to begin with? These are good questions that nobody knows the answer to. Nah, I'm just kidding. The answer is out there. I just need to find it. It is one of a handful of isotopes which \textit{could} be produced in significant amounts during an \textit{r} process scenario and which is known to undergo spontaneous fission, along with other californium and fermium isotopes. Furthermore, its half-life on the order of several days means that it could potentially make a significant impact on heating, especially at later times. The group of nuclei matching these criteria include {\Cf}, $^{257}$Es, and $^{260}$Md. Finally, according to the mass model and branching ratios they used in \cite{Zhu2018}, the other two nuclei seem like maybe they're more likely to $\beta$-decay, so {\Cf} it is. I talked to Samuel, and he says that everybody else predicts {\Cf}, too.

Will you not also want/need to mention your work on 290Fm? This {\Cf} is not all that neutron rich, nor is it far from the region most-commonly studied. So perhaps you should say and do a bit more work on Fermium. Even just to say that triaxiality is not important here is something and not nothing. The reason you began to study this particular nucleus, though, is that based on Trevor Sprouse's r process network calculations (which utilized Peter Moller's fission calculations) and depending on the specifics of the astrophysical conditions, fission terminates the r process in a region of the N-Z plane. 290Fm falls into the range identified in one of these calculations (it would be good to cite it) and was selected by Trevor as one which may be particularly significant. However, looking back over my emails, it seems like this is not a robust finding. Other predictions (like the ones I have from Samuel and Marius Eichler) seem to perhaps indicate that I'd be better off looking in the region for which 280Fm is the northeast corner, perhaps.

\section{Machine learning for r-process inputs}
Something that could be pretty doable and also moderately useful would be to use machine learning for fission yields to use in r-process network calculations. I got this idea from a guy at LLNL who gave a CMSE colloquium. He described a machine learning paradigm (he called it an ``Elevated Model'') in which they teach a deep learning network everything it could possibly care to know about a particular model - essentially, creating an emulator of the model. Then you kind of snip off the last couple of layers of your network, and reteach it (holding all but those last two layers constant), this time using experimental data. So by now you've gotten most of the physics intuition built into your emulator through the model training, and then for the physical insight your model misses (and perhaps to help reduce overfitting), you've taught it where the emulator goes wrong and what to do about it.

How can this help us fission folks? Well, right now the r-process network folks in the FIRE collaboration are using the GEF model for fission fragments and whatnot. It's basically a black box of magic that is super-overfitted and no one is completely sure how it works. BUT it's the best they've got for fission yields all across the region of interest, so that's what they use. My thought is to take this as a starting point (our ``model data''), and then use a combination of DFT calculations and also \textit{actual} experimental data as our experimental data. This could be an interesting opportunity to utilize your 290Fm results, along with anything else you've worked on.

Samuel worries (and I think, with justification) that we might not see any substantial improvements because the GEF is already so heavily overfitted. That, and that it might take a really long time (Leo's concerns are similar, plus he is concerned about having enough training data to do anything meaningful). That is why I'm thinking through what would need to be done, step by step, just to get a feel for a timeline of feasibility:
\begin{itemize}
	\item Finish a 290Fm calculation, just to have at least \textit{something} out there
	\item Figure out how to build a neural network where you can keep some layers fixed while selectively modifying others. I don't think that's something you could do in Scikit-learn; that you'd probably have to build custom (though fortunately you've already got something that might work!). Or you might be able to use TensorFlow (or Keras)
	\item Determine what observables you'd like (and are able) to include in your evaluation. For sure the locations of the peaks. Maybe also the width? Can you predict a full distribution from the same NN, or do you have to create a different one for each?
	\item Run the GEF for as many nuclei as you want to include in your evaluation
	\item Collect theoretical+experimental data you'll use to train your improved model
	\item Set aside a training set (Ideally you should include something that GEF clearly gets wrong compared to experiment, if possible)
\end{itemize}

The nice thing about this idea is that it can easily be extended or improved. Maybe some day Peter Moller has calculations across the landscape. You could just swap out the GEF data for Peter Moller's. Basically, it's just glorified interpolation, taking advantage of cheap models and combining/folding those with the more expensive, but more sparse, DFT and experimental results.