\chapter{Conclusions}\label{chap:Outlook}
%In this chapter, it would be great to talk to everyone you know (Witek, Samuel, Jhilam, Nicolas, Michal, and so on) to get a better feel for what kinds of issues need to be addressed next. You've already got sort of a rudimentary understanding (see your Google Keep note for starters), but it might be good to get some outsider perspective. This will be especially important as you start looking for postdocs, and \textit{especially} especially if you end up looking for postdocs in nuclear theory that aren't specifically nuclear fission.

The overarching goal of the project, in which this dissertation is a part, is to describe spontaneous fission yields in a self-consistent framework based on microscopic nuclear physics. We highlight spontaneous fission in three distinct and relatively-unexplored regions of the nuclear chart. New phenomena such as the observed asymmetry of fragments in the neutron-deficient sub-lead region, superheavy element synthesis and decay, and r-process nucleosynthesis force us to grapple with our understanding of fission and ultimately lead to greater insight and understanding of the fission process. New computational tools are supporting these developments by allowing us to use more efficient, more detailed models in our calculations.

In Chapter~\ref{chap:178Pt}, we estimated the peak of the fragment distribution corresponding to spontaneous fission of {\Pt}. In addition, we were able to qualitatively characterize properties of the fragments using properties of the PES, in a way that was compatible with experimental data.

In Chapter~\ref{chap:294Og}, we used Langevin dynamics to estimate the fission yields belonging to the superheavy element {\Og}. We predicted that this nucleus may fission according to an exotic form of fission called cluster emission, which is driven by the strong shell closures associated with $N=126$ and $Z=82$. We also took this calculation as an opportunity to test the robustness of microscopic fission calculations using Langevin dynamics, and we found that the peak was robustly predicted to within a few nucleons even as the inputs changed.

In Chapter~\ref{chap:rprocess}, we began using a new method for estimating fragment yields (see Appendix~\ref{append:Fragments}) to calculate fission fragment distributions that are relevant to the astrophysical r process. This method is designed to be efficient, which will allow it to be used for survey-level calculations that might later be used as inputs in r-process network calculations.

With that said, there is much still to accomplish on the way to a comprehensive microscopic description of spontaneous fission. This includes additional physics modeling as well as basic technical developments. The following few pages give an overview of some of these future prospects.

\section{Improved model fidelity}
Qualitatively and quantitatively, microscopic models are useful for understanding the complex physics of the fission process. To be useful for many applications, however, the level of quantitative agreement with experiment needs to improve significantly. Using the case of $^{240}$Pu as a benchmark~\cite{Sadhukhan2016}, the Langevin approach can predict spontaneous fission fragment yields to within around 30\% accuracy. Depending on the application, this number must be brought down to ${\sim}5\%$ to be useful.

Half-lives, which are an essential defining element of spontaneous fission, are especially difficult to estimate. Assuming that the minimum action is computed to the same 30\% accuracy as the yields (which is admittedly a rough estimate, since these quantities depend on two different regions of the PES with drastically different dynamics), then, because of the exponential dependence of the half-life on the action, the half-life may be off by several orders of magnitude.

%... Improving precision (and scission!) and quantifying uncertainties will also prepare the way for other observables, such as spin and kinetic energies.

% There are maybe better ways to compute $E_0$? \verb|\cite{???}| But I don't know what it is, so I'll just not mention it

Since fission lifetimes are largely determined by the topography of the PES through which the nucleus must tunnel, it is important to improve predictions of both the potential energy and the collective inertia. The perturbative inertia is easy and fast to compute, but not expected to be as reliable as the non-perturbative inertia. And as discussed in Section~\ref{sect:M_numerical}, the finite difference formulae from which the non-perturbative inertia is often computed are subject to errors due to level crossings and numerical artifacts. One tool that would eliminate the need for finite-difference derivatives is automatic differentiation. Automatic differentiation essentially keeps track of arithmetic operations as they are performed and combines their derivatives according to the chain rule. Because this is performed as the code runs, it adds almost no additional runtime and no numerical error. Once implemented, automatic differentiation would allow derivatives to be computed with machine precision, without the need to compute any additional nearby points. The chief drawback is that automatic differentiation can be difficult to implement.

As for the potential energy, the current state-of-the-art functional used for fission - UNEDF1 - tends to underestimate the height of saddle points on the PES, which in turn has the effect of artificially reducing half-lives. One might suggest refitting the functional using additional fission data, but unfortunately, the UNEDF2 project~\cite{Kortelainen2014} concluded essentially that Skyrme-like functionals have been pushed to their limits. A next-generation set of functionals based on the UNEDF project has been proposed which uses an expansion of the density matrix and a link to modern effective forces, both of which can be systematically improved~\cite{NavarroPerez2018}. This set of functionals has not been extensively tested, save for some initial benchmarking, so it is not clear whether there will be any added benefit to using them. It may be that new functionals such as these will better reproduce fission data, but it should be noted that, while the calculations presented here would not be affected, introducing an explicit density-dependence can cause problems for beyond-mean field corrections~\cite{duguet2009, dobaczewski2007, anguiano2001}. %It also turns out to be a headache to work with, making convergence quite a challenge sometimes (any cases in particular, like for highly-deformed or heavy or octupole-deformed nuclei or something?).

It was assumed throughout this dissertation that fissioning nuclei behave as superfluids maintained at temperature $T=0$. However, a proper description of fission from an excited compound nucleus as in the case of Chapter~\ref{chap:178Pt}, or neutron-induced fission as in Chapter~\ref{chap:rprocess}, requires a finite-temperature formalism. This is discussed in Appendix~\ref{append:TD-ATDHFB}.

A known problem when constructing PESs is the existence of discontinuities, which occur when one reduces a complex system with hundreds of degrees of freedom into only a few collective coordinates~\cite{dubray2012}. This can obscure important physics, such as saddle point heights and the complex physics of scission. Brute force computation with a larger number of collective coordinates (such as we did in Chapter~\ref{chap:294Og}) will become more accessible as computers continue to become more powerful. However, we might see a faster turnaround on investment if we start using collective coordinates that better describe the shape of fissioning nuclei. For instance, a set of coordinates $D, \xi$ was proposed as an alternative to the multipole moments $Q_{20}$ and $Q_{30}$ in~\cite{younes2012}, where $D$ is the distance between prefragment centers of mass and $\xi = (A_R-A_L)/A$ characterizes the fragment asymmetry. Using these coordinates, the authors found that they were better able to describe scission configurations than if they had used multipole moments instead. % Would non-geometric collective coordinates be better-suited for fission? Like some kind of interaction energy that is unique to that process?

% To that end, it is worth mentioning as well Fragment identification (our localization paper, Marc Verriere's method; you might also mention that this is not an issue in TDDFT, but there you've only got one single fragment pair) - I don't think Marc's work is published yet, though, and I don't have anything better to suggest right now

Finally, to make the calculations described in this dissertation fully self-consistent will require the development of a self-consistent description of dissipation. In addition to contributing to a more reliable model, it may be that a better understanding of the exchange mechanisms between intrinsic and collective degrees of freedom could allow one to more reliably model fragment kinetic and excitation energies. The topic of dissipative motion in quantum systems is being studied theoretically~\cite{Koch2008, Lacroix2008, Hupin2010, Sargsyan2010, Bulgac2018a}, but the ideas are not yet sufficiently well-developed for quantitative fission calculations.

% A proper understanding of the friction tensor (as it were) may also help with the calculation of neutron emissions, according to this: ``The  influence  of  friction  on  the  fission  rate  was studied in papers [57, 58]. It was shown there that dissipative  effects  might  lead  to  emission  of  more  neutrons from fissioning nuclei than what is predicted by equilibrium  statistical  models.'' (\verb|https://link.springer.com/article/10.1134%2FS1063779610020012|).

% Some other papers you can look at for this might be \verb|https://doi.org/10.1142/S0218301398000105|, \verb|https://doi.org/10.1016/j.physletb.2007.09.072|, and \verb|https://doi.org/10.1016/0375-9474(79)90559-1|.

% I don't think anyone is really looking to do this, beyond the TXE+TKE info which they can then plug into their Hauser-Feshbach codes
%\section{Computing other fission observables}
%It is tempting and exciting to push a model to its limits, and to see how generally it can be used. In this case, that means figuring out how to predict other observables in a self-consistent framework beyond half-lives and fragment yields. These might include fragment energetics (kinetic and excitation energies), fragment angular distributions, spins, prompt neutron multiplicities, prompt neutron and gamma energy spectra. It may soon be feasible to accurately predict fragment energies within the adiabatic approximation~\cite{Younes2011}; for everything else, the issue of how to compute these observables in a self-consistent framework is still an open question. In fact, many of these (especially particle emissions) may turn out to be forever beyond the scope of self-consistent models, delving deep into the realm of statistical physics. In that case, it would be interesting to see how these statistical models perform with self-consistent inputs.

% Also, temp-dependence...


\section{Computational tools}
As we look for and find new ways to study and apply fission, we might consider the availability of new computational tools, such as machine learning and GPUs.

As stated earlier, our biggest bottleneck is the calculation of the PES. The expense of calculations has the side effect of forcing us to use small collective spaces with relatively-few collective variables, which may mean a loss of important physics. Therefore, it is worth investing in methods to reduce the burden of PES calculations. One potential solution is a machine learning method called speculative sampling. Speculative sampling works well for problems that have an expensive computational model, and calculations that must be performed for many inputs or in a large parameter space. The essence of this technique is to create an emulator for your model using machine learning, then initiate many calculations using the expensive model and terminate early any calculations which do not appear to diverge strongly from the emulator. In this way, we don't waste time calculating things that the emulator can already predict, and instead focus our efforts on those portions of the PES which provide new information. After this set of calculations converges, the emulator can be retrained using the new data and the procedure is repeated.

This could be implemented for our problem in the following way: Start by using Monte Carlo sampling to begin constructing a PES in a large collective space using a traditional DFT solver, and then use those results to train a PES emulator. Then use Monte Carlo sampling to start a new round of PES calculations, only this time compare the results from the DFT solver in real-time to the emulator's prediction. As long as the DFT solver diverges from the prediction it should be allowed to continue, but once it appears that the DFT calculation agrees with the prediction the calculation terminates. The emulator should then be retrained using the new data, and the feedback cycle continued until the PES is predicted sufficiently-well by the emulator. Although traditional simulations will likely continue to be performed on traditional CPUs, machine learning performs well on GPUs. That means this technique will be able to take advantage of many of the newest architectures.

For microscopic calculations to become practical for r-process calculations, though, would require an efficient way of estimating fission properties on a large scale, ideally covering the whole nuclear chart. To do this using conventional calculations will still be a tremendous undertaking, even with the potential speedup from speculative sampling. Another machine learning paradigm called ``transfer learning'' might be useful here. In transfer learning, one starts with a set of data which is large, but imperfect. For fission, this might mean yields computed globally via a phenomenological model such as GEF~\cite{Schmidt2016}. This data is used to train a model, such as an artificial neural network, so that essentially one has an emulator that understands the physics of the cheap model. This knowledge is then \textit{transferred} (hence the name) to a new model with new data, which in our case might be experimental data or even the results from detailed microscopic calculations. In a neural network with many layers, this transfer might take place by retraining the model while holding all but the last two or three of layers constant. In our example, the first several layers contain most of the basic physics of fission, and then the last few layers contain corrections and improvements to the simple model.

% See here for where you got the ideas: https://bout.llnl.gov/content/assets/docs/workshops/2018/2018-08-17_Spears.pdf

\section{Outlook}
It is an exciting time to study fission. New phenomena are being discovered as fission experiments are performed in unfamiliar regions of the nuclear landscape, and there is a healthy feedback loop between theory and experiment thanks in great part to modern computational tools. It is, in many ways, the culmination of years of theoretical development, but there is still much to be done.