\chapter{Outlook}\label{chap:Outlook}
%In this chapter, it would be great to talk to everyone you know (Witek, Samuel, Jhilam, Nicolas, Michal, and so on) to get a better feel for what kinds of issues need to be addressed next. You've already got sort of a rudimentary understanding (see your Google Keep note for starters), but it might be good to get some outsider perspective. This will be especially important as you start looking for postdocs, and \textit{especially} especially if you end up looking for postdocs in nuclear theory that aren't specifically nuclear fission.

The overarching goal of the project, in which this dissertation is a part, is to describe spontaneous fission observables in a fully self-consistent framework based on fundamental nuclear physics. We highlight spontaneous fission in three relatively-new and distinct regions of the nuclear chart. By studying the formation of fragments via the nucleon localization function and the primary fragment yields, we learn that fission is more complicated than previously thought. Even today there are still models which show a clear preference for $^{132}$Sn \cite{Carjan2017}. New phenomena such as the observed asymmetry of fragments in the neutron-deficient sub-lead region, superheavy element synthesis and decay, and r-process nucleosynthesis force us to grapple with our understanding of fission and ultimately lead to greater insight and understanding.

With that said, there is much still to be done on the way to a comprehensive microscopic description of spontaneous fission. This includes additional physics modeling as well as technical developments. The following few pages give an overview of some of these future prospects.

\section{Improved model fidelity}
Qualitatively, microscopic models are quite useful for understanding the complex physics of the fission process. To be useful for many applications, however, the level of quantitative agreement with experiment needs to improve significantly. Using the case of $^{240}$Pu as a benchmark \cite{Sadhukhan2016}, our model can predict fragment yields to within around 30\% accuracy. Depending on the application, this number needs to be brought down to the single digits to be useful.

This is especially crucial for estimating half-lives, which are an essential defining element of spontaneous fission. Assuming the same 30\% accuracy for the computed action, then, because of the exponential dependence of the half-life on the action, the half-life may be off by up to 5 orders of magnitude.

%... Improving precision (and scission!) and quantifying uncertainties will also prepare the way for other observables, such as spin and kinetic energies.

% There are maybe better ways to compute $E_0$? \verb|\cite{???}| But I don't know what it is, so I'll just not mention it

Since half-lives are largely determined by the topography of the PES through which the nucleus must tunnel, it is important to improve predictions of both the potential energy and the collective inertia. The perturbative inertia is easy and fast to compute, but not terribly reliable. The non-perturbative inertia can certainly do better, but as discussed in Section \ref{sect:M_numerical}, the finite difference formulae from which it is currently computed are subject to errors due to level crossings and numerical artifacts . One tool that would eliminate the need for finite-difference derivatives is automatic differentiation. Automatic differentiation essentially keeps track of arithmetic operations as they are performed and combines their according to the chain rule. Because this is performed as the code runs, it adds almost no additional runtime and no numerical error. Once implemented, automatic differentiation would allow derivatives to be computed with machine precision, without the need to also compute nearby points. The chief drawback is that automatic differentiation can be tricky to implement.

As for the potential energy, the current state-of-the-art functional used for fission - UNEDF1 - seems to underestimate the height of saddle points on the PES, which in turn has the effect of reducing half-lives. One might suggest refitting the functional to using additional fission data, but unfortunately, the UNEDF2 project \cite{Kortelainen2014} showed essentially that Skyrme-like functionals have been pushed to their limits. A next-generation set of functionals based on the UNEDF procedure has been proposed which uses an expansion of the density matrix and a link to modern effective forces, both of which can be systematically improved \cite{NavarroPerez2018}. This set of functionals has not been extensively tested, save for some initial benchmarking, so it is not clear whether there will be any added benefit to using them. Furthermore, while this would not affect the calculations presented here, introducing an explicit density-dependence can cause problems for beyond-mean field corrections. %It also turns out to be a headache to work with, making convergence quite a challenge sometimes (any cases in particular, like for highly-deformed or heavy or octupole-deformed nuclei or something?).

Experience has shown that the current collective coordinates do not well-describe PESs in the region of {\Pt} nor in the r-process region. Trying to reduce a complex system with hundreds of degrees of freedom into only a few collective coordinates creates artificial discontinuities and other artifacts, while at times missing important physics (such as around scission). Brute force computation with a larger number of collective coordinates (such as we did in Chapter \ref{chap:294Og}) will become more accessible as computers continue to become more powerful. However, we might see a faster turnaround on investment if we start using collective coordinates that better describe the physics of fission. For instance, a set of coordinates $D, \xi$ was proposed as an alternative to the traditional $Q_{20}, Q_{30}$ coordinates in \cite{Younes2012}, where $D$ is the distance between prefragment centers of mass and $\xi = (A_R-A_L)/A$ characterizes the fragment asymmetry. Using these coordinates, the authors found that they were better able to describe scission configurations than if they had used multipole moments instead. % Would non-geometric collective coordinates be better-suited for fission? Like some kind of interaction energy that is unique to that process?

% To that end, it is worth mentioning as well Fragment identification (our localization paper, Marc Verriere's method; you might also mention that this is not an issue in TDDFT, but there you've only got one single fragment pair) - I don't think Marc's work is published yet, though, and I don't have anything better to suggest right now

To make the calculations described in this dissertation fully self-consistent will require the development of a self-consistent description of dissipation. In addition to contributing to a more reliable model, it may be that a better understanding of the exchange mechanisms between intrinsic and collective degrees of freedom might allow one to more reliably model fragment kinetic and excitation energies and prompt neutron emissions. The topic of dissipative motion in quantum systems is being studied theoretically \cite{Koch2008, Lacroix2008, Hupin2010, Sargsyan2010}, but the ideas are not yet sufficiently well-developed for our use.

% A proper understanding of the friction tensor (as it were) may also help with the calculation of neutron emissions, according to this: ``The  influence  of  friction  on  the  fission  rate  was studied in papers [57, 58]. It was shown there that dissipative  effects  might  lead  to  emission  of  more  neutrons from fissioning nuclei than what is predicted by equilibrium  statistical  models.'' (\verb|https://link.springer.com/article/10.1134%2FS1063779610020012|).

% Some other papers you can look at for this might be \verb|https://doi.org/10.1142/S0218301398000105|, \verb|https://doi.org/10.1016/j.physletb.2007.09.072|, and \verb|https://doi.org/10.1016/0375-9474(79)90559-1|.


\section{Computing other observables/new physics}
It is tempting and exciting to push a model to its limits, and to see how generally it can be used. In this case, that means figuring out how to predict other observables in a self-consistent framework beyond half-lives and fragment yields. These might include fragment energetics (kinetic and excitation energies), fragment angular distributions, spins, prompt neutron multiplicities, prompt neutron and gamma energy spectra. It may soon be feasible to accurately predict fragment energies within the adiabatic approximation \cite{Younes2011}; for everything else, the issue of how to compute these in a self-consistent framework is still an open question. In fact, many of these (especially particle emissions) may turn out to be forever beyond the scope of self-consistent models, delving deep into the realm of statistical physics. Nevertheless, it would be interesting to see how these statistical models perform with self-consistent inputs.

% Also, temp-dependence...


\section{New tools}
Finally, we can look to other regions on the nuclear chart to gain a global perspective of fission. To assist in this exploration, as we look for and find new ways to study and apply fission, we might consider the availability of new tools (like machine learning)

\subsection{Large-scale calculations with machine learning}
Something that could be pretty doable and also moderately useful would be to use machine learning for fission yields to use in r-process network calculations. I got this idea from a guy at LLNL who gave a CMSE colloquium. He described a machine learning paradigm (he called it an ``Elevated Model'') in which they teach a deep learning network everything it could possibly care to know about a particular model - essentially, creating an emulator of the model. Then you kind of snip off the last couple of layers of your network, and reteach it (holding all but those last two layers constant), this time using experimental data. So by now you've gotten most of the physics intuition built into your emulator through the model training, and then for the physical insight your model misses (and perhaps to help reduce overfitting), you've taught it where the emulator goes wrong and what to do about it.

How can this help us fission folks? Well, right now the r-process network folks in the FIRE collaboration are using the GEF model for fission fragments and whatnot. It's basically a black box of magic that is super-overfitted and no one is completely sure how it works. BUT it's the best they've got for fission yields all across the region of interest, so that's what they use. My thought is to take this as a starting point (our ``model data''), and then use a combination of DFT calculations and also \textit{actual} experimental data as our experimental data. This could be an interesting opportunity to utilize your 290Fm results, along with anything else you've worked on.

Samuel worries (and I think, with justification) that we might not see any substantial improvements because the GEF is already so heavily overfitted. That, and that it might take a really long time (Leo's concerns are similar, plus he is concerned about having enough training data to do anything meaningful). That is why I'm thinking through what would need to be done, step by step, just to get a feel for a timeline of feasibility:

\begin{itemize}
	\item Finish a 290Fm calculation or something nearby, just to have at least \textit{something} out there
	\item Figure out how to build a neural network where you can keep some layers fixed while selectively modifying others. I don't think that's something you could do in Scikit-learn; that you'd probably have to build custom (though fortunately you've already got something that might work!). Or you might be able to use TensorFlow (or Keras)
	\item Determine what observables you'd like (and are able) to include in your evaluation. For sure the locations of the peaks. Maybe also the width? Can you predict a full distribution from the same NN, or do you have to create a different one for each?
	\item Run the GEF for as many nuclei as you want to include in your evaluation
	\item Collect theoretical+experimental data you'll use to train your improved model
	\item Set aside a training set (Ideally you should include something that GEF clearly gets wrong compared to experiment, if possible)
\end{itemize}

The nice thing about this idea is that it can easily be extended or improved. Maybe some day Peter Moller has calculations across the landscape. You could just swap out the GEF data for Peter Moller's. Basically, it's just glorified interpolation, taking advantage of cheap models and combining/folding those with the more expensive, but more sparse, DFT and experimental results.

However, just based on Figure 1 of \cite{Vassh2018} and the preliminary yields I have for 266Cm and 290Fm, it doesn't look like we'd be adding too much new information. It looks to me like they already got the basic shape of the yields, and I don't know if I'd do much better with the localization fragment ID approach I'd been considering using.

\section{Review and conclusions}
It's an exciting time to study fission. The computational tools at our disposal have allowed for some very difficult calculations to be performed. It is, in many ways, the culmination of years of theoretical development. There is still much to be done.