\chapter{Outlook}\label{chap:Outlook}
%In this chapter, it would be great to talk to everyone you know (Witek, Samuel, Jhilam, Nicolas, Michal, and so on) to get a better feel for what kinds of issues need to be addressed next. You've already got sort of a rudimentary understanding (see your Google Keep note for starters), but it might be good to get some outsider perspective. This will be especially important as you start looking for postdocs, and \textit{especially} especially if you end up looking for postdocs in nuclear theory that aren't specifically nuclear fission.

The overarching goal of the project, in which this dissertation is a part, is to describe spontaneous fission observables in a fully self-consistent framework based on fundamental nuclear physics. We highlight spontaneous fission in three relatively-new and distinct regions of the nuclear chart. By studying the formation of fragments via the nucleon localization function and the primary fragment yields, we learn that fission is more complicated than previously thought. Even today there are still models which show a clear preference for $^{132}$Sn~\cite{Carjan2017}. New phenomena such as the observed asymmetry of fragments in the neutron-deficient sub-lead region, superheavy element synthesis and decay, and r-process nucleosynthesis force us to grapple with our understanding of fission and ultimately lead to greater insight and understanding.

With that said, there is much still to be done on the way to a comprehensive microscopic description of spontaneous fission. This includes additional physics modeling as well as technical developments. The following few pages give an overview of some of these future prospects.

\section{Improved model fidelity}
Qualitatively, microscopic models are quite useful for understanding the complex physics of the fission process. To be useful for many applications, however, the level of quantitative agreement with experiment needs to improve significantly. Using the case of $^{240}$Pu as a benchmark~\cite{Sadhukhan2016}, our model can predict fragment yields to within around 30\% accuracy. Depending on the application, this number needs to be brought down to the single digits to be useful.

This is especially crucial for estimating half-lives, which are an essential defining element of spontaneous fission. Assuming the same 30\% accuracy for the computed action, then, because of the exponential dependence of the half-life on the action, the half-life may be off by up to 5 orders of magnitude.

%... Improving precision (and scission!) and quantifying uncertainties will also prepare the way for other observables, such as spin and kinetic energies.

% There are maybe better ways to compute $E_0$? \verb|\cite{???}| But I don't know what it is, so I'll just not mention it

Since half-lives are largely determined by the topography of the PES through which the nucleus must tunnel, it is important to improve predictions of both the potential energy and the collective inertia. The perturbative inertia is easy and fast to compute, but not terribly reliable. The non-perturbative inertia can certainly do better, but as discussed in Section~\ref{sect:M_numerical}, the finite difference formulae from which it is currently computed are subject to errors due to level crossings and numerical artifacts . One tool that would eliminate the need for finite-difference derivatives is automatic differentiation. Automatic differentiation essentially keeps track of arithmetic operations as they are performed and combines their according to the chain rule. Because this is performed as the code runs, it adds almost no additional runtime and no numerical error. Once implemented, automatic differentiation would allow derivatives to be computed with machine precision, without the need to also compute nearby points. The chief drawback is that automatic differentiation can be tricky to implement.

As for the potential energy, the current state-of-the-art functional used for fission - UNEDF1 - seems to underestimate the height of saddle points on the PES, which in turn has the effect of reducing half-lives. One might suggest refitting the functional to using additional fission data, but unfortunately, the UNEDF2 project~\cite{Kortelainen2014} showed essentially that Skyrme-like functionals have been pushed to their limits. A next-generation set of functionals based on the UNEDF procedure has been proposed which uses an expansion of the density matrix and a link to modern effective forces, both of which can be systematically improved~\cite{NavarroPerez2018}. This set of functionals has not been extensively tested, save for some initial benchmarking, so it is not clear whether there will be any added benefit to using them. Furthermore, while this would not affect the calculations presented here, introducing an explicit density-dependence can cause problems for beyond-mean field corrections. %It also turns out to be a headache to work with, making convergence quite a challenge sometimes (any cases in particular, like for highly-deformed or heavy or octupole-deformed nuclei or something?).

Experience has shown that the current collective coordinates do not well-describe PESs in the region of {\Pt} nor in the r-process region. Trying to reduce a complex system with hundreds of degrees of freedom into only a few collective coordinates creates artificial discontinuities and other artifacts, while at times missing important physics (such as around scission). Brute force computation with a larger number of collective coordinates (such as we did in Chapter~\ref{chap:294Og}) will become more accessible as computers continue to become more powerful. However, we might see a faster turnaround on investment if we start using collective coordinates that better describe the physics of fission. For instance, a set of coordinates $D, \xi$ was proposed as an alternative to the traditional $Q_{20}, Q_{30}$ coordinates in~\cite{Younes2012}, where $D$ is the distance between prefragment centers of mass and $\xi = (A_R-A_L)/A$ characterizes the fragment asymmetry. Using these coordinates, the authors found that they were better able to describe scission configurations than if they had used multipole moments instead. % Would non-geometric collective coordinates be better-suited for fission? Like some kind of interaction energy that is unique to that process?

% To that end, it is worth mentioning as well Fragment identification (our localization paper, Marc Verriere's method; you might also mention that this is not an issue in TDDFT, but there you've only got one single fragment pair) - I don't think Marc's work is published yet, though, and I don't have anything better to suggest right now

To make the calculations described in this dissertation fully self-consistent will require the development of a self-consistent description of dissipation. In addition to contributing to a more reliable model, it may be that a better understanding of the exchange mechanisms between intrinsic and collective degrees of freedom might allow one to more reliably model fragment kinetic and excitation energies and prompt neutron emissions. The topic of dissipative motion in quantum systems is being studied theoretically~\cite{Koch2008, Lacroix2008, Hupin2010, Sargsyan2010}, but the ideas are not yet sufficiently well-developed.

% A proper understanding of the friction tensor (as it were) may also help with the calculation of neutron emissions, according to this: ``The  influence  of  friction  on  the  fission  rate  was studied in papers [57, 58]. It was shown there that dissipative  effects  might  lead  to  emission  of  more  neutrons from fissioning nuclei than what is predicted by equilibrium  statistical  models.'' (\verb|https://link.springer.com/article/10.1134%2FS1063779610020012|).

% Some other papers you can look at for this might be \verb|https://doi.org/10.1142/S0218301398000105|, \verb|https://doi.org/10.1016/j.physletb.2007.09.072|, and \verb|https://doi.org/10.1016/0375-9474(79)90559-1|.


\section{Computing other fission observables}
It is tempting and exciting to push a model to its limits, and to see how generally it can be used. In this case, that means figuring out how to predict other observables in a self-consistent framework beyond half-lives and fragment yields. These might include fragment energetics (kinetic and excitation energies), fragment angular distributions, spins, prompt neutron multiplicities, prompt neutron and gamma energy spectra. It may soon be feasible to accurately predict fragment energies within the adiabatic approximation~\cite{Younes2011}; for everything else, the issue of how to compute these observables in a self-consistent framework is still an open question. In fact, many of these (especially particle emissions) may turn out to be forever beyond the scope of self-consistent models, delving deep into the realm of statistical physics. In that case, it would be interesting to see how these statistical models perform with self-consistent inputs.

% Also, temp-dependence...


\section{New computational tools}
Finally, as we look for and find new ways to study and apply fission, we might consider the availability of new tools, such as machine learning and GPUs.

As stated earlier, our biggest bottleneck is the calculation of the PES. Aside from the expense, this has the side effect of forcing us to use small collective spaces with relatively-few collective variables, which may mean a loss of important physics. Therefore, it is worth investing in methods to reduce the burden of PES calculations. One potential solution is a machine learning method called speculative sampling. Speculative sampling works well for problems that have an expensive computational model, and calculations that must be performed for many inputs or in a large parameter space. The essence of the paradigm is to take a machine-learned emulator for your model, initiate many calculations using your expensive model calculations, and then terminate any calculations which do not appear to diverge strongly from the emulator. In this way, we can cut down on the number of calculations performed, and focus our efforts only on those which provide new information for the emulator.

This could be implemented for our problem in the following way: use Monte Carlo sampling to begin constructing a PES in a large collective space using a traditional DFT solver, and then use those results to train a PES emulator. Then use Monte Carlo sampling to start a new round of PES calculations, only this time compare the results from the DFT solver in real-time to the emulator's prediction. As long as the DFT solver diverges from the prediction it should be allowed to continue, but once it appears that the DFT calculation agrees with the prediction the calculation terminates. The emulator should then be retrained using the new data, and the feedback cycle continued until the PES is sufficiently well-predicted by the emulator.

Although traditional simulations will likely continue to be performed on traditional CPUs, machine learning performs well on GPUs, which means taking advantage of many of the newest architectures.

For microscopic calculations to become practical for r-process calculations, though, would require an efficient way of estimating fission properties on a large scale, ideally covering the whole nuclear chart. To do this using conventional calculations will still be a tremendous undertaking, even with the potential speedup from speculative sampling. Another machine learning paradigm called ``transfer learning'' might be useful here.

In transfer learning, one starts with a set of data which is large, but imperfect. For fission, this might mean yields computed globally via a phenomenological model such as GEF~\cite{Schmidt2016}. This data is used to train a model, such as an artificial neural network, so that essentially one has an emulator that understands the physics of the cheap model. This knowledge is then \textit{transferred} to a new model with new data, which in our case might be experimental data or even the results from microscopic calculations. In a neural network with many layers, this transfer might take place by retraining the model while holding all but the last two or three of layers constant. In our example, the first several layers contain most of the basic physics of fission, and then the last couple layers contain corrections to the simple model.

% See here for where you got the ideas: https://bout.llnl.gov/content/assets/docs/workshops/2018/2018-08-17_Spears.pdf

\section{Conclusion}
It's an exciting time to study fission. New phenomena are being discovered as fission experiments are performed in unfamiliar regions of the nuclear landscape, and there is a healthy feedback loop between theory and experiment thanks in great part to modern computational tools. It is, in many ways, the culmination of years of theoretical development, but there is still much to be done.