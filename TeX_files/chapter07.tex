\chapter{Outlook}\label{chap:Outlook}
%In this chapter, it would be great to talk to everyone you know (Witek, Samuel, Jhilam, Nicolas, Michal, and so on) to get a better feel for what kinds of issues need to be addressed next. You've already got sort of a rudimentary understanding (see your Google Keep note for starters), but it might be good to get some outsider perspective. This will be especially important as you start looking for postdocs, and \textit{especially} especially if you end up looking for postdocs in nuclear theory that aren't specifically nuclear fission.

Having discussed several different results, it is worth taking some time to reflect.

The overarching goal of the project, in which this dissertation is a part, is to describe spontaneous fission observables in a fully self-consistent framework based on fundamental nuclear physics. At this stage, we are able to estimate half-lives and primary fragment distributions [to around X\% accuracy, for the nuclei with which comparison to experiment is possible].

\section{Perspectives for future model development}

\subsection{Numerical challenges}
We can hope to achieve greater fidelity in our calculations by

We definitely need a better handle on the inertia. The perturbative inertia is easy to compute, but not terribly reliable. The non-perturbative inertia can certainly do better, but as it is computed now (using finite differences) it is subject to numerical artifacts and instabilities (dependent on the level of convergence of the individual densities, the coefficient multipliers, different basis sizes) as well as to actual physics, such as level crossings which manifest in projections from a higher-dimensional space. One tool that has been suggested which would eliminate the need for finite-difference derivatives is automatic differentiation \verb|\cite{???}|.

UNEDF1 seems to underestimate fission barrier heights (artificial though the concept may be; the main impact is probably that lifetimes are underestimated). It also turns out to be a headache to work with, making convergence quite a challenge sometimes (any cases in particular, like for highly-deformed or heavy or octupole-deformed nuclei or something?). Better functionals might hope to better capture the physics, and one can hope they are easier to work with. (here you could mention the DME EDFs; unfortunately, introducing density-dependence causes problems for beyond-mean field corrections)

The construction of a PES is fraught with ambiguities. Trying to reduce a complex system with hundreds of degrees of freedom into only a few collective coordinates creates artificial discontinuities and other artifacts, while at times missing important physics. Brute force computation with a larger number of collective coordinates (such as we did in Chapter \ref{chap:294Og}) will become more accessible as computers continue to become more powerful. However, we might see a faster turnaround on our investment if we start using collective coordinates that better describe the physics of fission. For instance, a set of coordinates $D, \xi$ was proposed as an alternative to the traditional $Q_{20}, Q_{30}$ coordinates in \cite{Younes2012}, where $D$ is the distance between prefragment centers of mass and $\xi = (A_R-A_L)/A$ characterizes the fragment asymmetry. Using these coordinates, the authors found that they were better able to describe scission configurations than if they had used multipole moments instead.

To that end, it is worth mentioning as well Fragment identification (our localization paper, Marc Verriere's method; you might also mention that this is not an issue in TDDFT, but there you've only got one single fragment pair)

\subsection{Self-consistent stochastic dynamics}
To make the calculations we've performed fully self-consistent, we will need to use a microscopic/self-consistent description for dissipation. This is the mechanism which exchanges between intrinsic and collective degrees of freedom, but we handle it in a very ad hoc way with parameters which are fitted instead of determined systematically through some theory. Solving this problem will probably help us with the energetics of fragments (TKE and E* at the same time!)

A proper understanding of the friction tensor (as it were) may also help with the calculation of neutron emissions, according to this: ``The  influence  of  friction  on  the  fission  rate  was studied in papers [57, 58]. It was shown there that dissipative  effects  might  lead  to  emission  of  more  neutrons from fissioning nuclei than what is predicted by equilibrium  statistical  models.'' (\verb|https://link.springer.com/article/10.1134%2FS1063779610020012|).

Some other papers you can look at for this might be \verb|https://doi.org/10.1142/S0218301398000105|, \verb|https://doi.org/10.1016/j.physletb.2007.09.072|, and \verb|https://doi.org/10.1016/0375-9474(79)90559-1|.

\section{Perspectives for future physics applications}

\subsection{Half-lives}

Although an approach was outlined for computing spontaneous fission half-lives in Chapter \ref{chap:Model}, no practical results have yet been obtained using this method. The biggest difficulty here is the exponential dependence of the half-life on the action, which is in turn a function of the potential energy, the collective inertia, and the zero-point energy. The numerical improvements outlined above will all serve to ameliorate the situation; additionally, there are maybe better ways to compute $E_0$? \verb|\cite{???}|

\subsection{Collective variables}

Experience has shown that the current collective coordinates do not well-describe PESs in the region of {\Pt} nor in the r-process region (you can tell because of the sharp cutoffs that indicate scission, and the ridiculously-high saddle points on the neutron-rich side). Better collective coordinates would certainly lead to a better understanding of these nuclei and the way in which they fission. It may also lead to a better understanding of fission overall.

\subsection{Collective inertia in neutron-deficient sublead region}

At the time the {\Hg} result was published, microscopic models were not yet sufficiently developed to compute the full fission dynamics. What would the collective energy show? [time-dependent calculations might be interesting to see, as well, but no need to mention that]

\subsection{Predicting other experimental observables}

Furthermore, there are more experimental observables that we should try to predict (refer to Andreyev's review to see what other observables can currently be measured). These include energetics (TKE and E*, for we have only begun to scratch the surface here), angular momentum, prompt neutron multiplicities (is that within the scope of these self-consistent models?), prompt neutron and gamma energy spectra (getting harder; these are usually handled via statistical models; see intro to \cite{Schmidt2018} for some references), level densities?, and probably more but my mind is blanking. How to compute these in a self-consistent framework is still an open question. See also the outlook in Nicolas' review.

Some methods (such as Walid's, TDDFT, and possibly also this GCM method) are starting to estimate fragment energetics (kinetic and excitation energies). Down the line, there are others who try to predict neutron multiplicities and goodness knows what else using Hauser-Feshbach models and such (FREYA and more). These regions are still disconnected. Of course, these methods still need major refinements in order to better reflect experimental data. Some ideas currently in the pipeline for improving the models are:

\subsection{Large-scale calculations with machine learning}
Something that could be pretty doable and also moderately useful would be to use machine learning for fission yields to use in r-process network calculations. I got this idea from a guy at LLNL who gave a CMSE colloquium. He described a machine learning paradigm (he called it an ``Elevated Model'') in which they teach a deep learning network everything it could possibly care to know about a particular model - essentially, creating an emulator of the model. Then you kind of snip off the last couple of layers of your network, and reteach it (holding all but those last two layers constant), this time using experimental data. So by now you've gotten most of the physics intuition built into your emulator through the model training, and then for the physical insight your model misses (and perhaps to help reduce overfitting), you've taught it where the emulator goes wrong and what to do about it.

How can this help us fission folks? Well, right now the r-process network folks in the FIRE collaboration are using the GEF model for fission fragments and whatnot. It's basically a black box of magic that is super-overfitted and no one is completely sure how it works. BUT it's the best they've got for fission yields all across the region of interest, so that's what they use. My thought is to take this as a starting point (our ``model data''), and then use a combination of DFT calculations and also \textit{actual} experimental data as our experimental data. This could be an interesting opportunity to utilize your 290Fm results, along with anything else you've worked on.

Samuel worries (and I think, with justification) that we might not see any substantial improvements because the GEF is already so heavily overfitted. That, and that it might take a really long time (Leo's concerns are similar, plus he is concerned about having enough training data to do anything meaningful). That is why I'm thinking through what would need to be done, step by step, just to get a feel for a timeline of feasibility:

\begin{itemize}
	\item Finish a 290Fm calculation or something nearby, just to have at least \textit{something} out there
	\item Figure out how to build a neural network where you can keep some layers fixed while selectively modifying others. I don't think that's something you could do in Scikit-learn; that you'd probably have to build custom (though fortunately you've already got something that might work!). Or you might be able to use TensorFlow (or Keras)
	\item Determine what observables you'd like (and are able) to include in your evaluation. For sure the locations of the peaks. Maybe also the width? Can you predict a full distribution from the same NN, or do you have to create a different one for each?
	\item Run the GEF for as many nuclei as you want to include in your evaluation
	\item Collect theoretical+experimental data you'll use to train your improved model
	\item Set aside a training set (Ideally you should include something that GEF clearly gets wrong compared to experiment, if possible)
\end{itemize}

The nice thing about this idea is that it can easily be extended or improved. Maybe some day Peter Moller has calculations across the landscape. You could just swap out the GEF data for Peter Moller's. Basically, it's just glorified interpolation, taking advantage of cheap models and combining/folding those with the more expensive, but more sparse, DFT and experimental results.

However, just based on Figure 1 of \cite{Vassh2018} and the preliminary yields I have for 266Cm and 290Fm, it doesn't look like we'd be adding too much new information. It looks to me like they already got the basic shape of the yields, and I don't know if I'd do much better with the localization fragment ID approach I'd been considering using.

\section{Review and conclusions}
